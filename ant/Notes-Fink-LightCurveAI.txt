# data extraction ##############################################################

# @livy
git pull; hadoop fs -put -f ../src/work/LightCurves/extractLC.py

# @janusgraph
lomikel_hbase -s ../src/work/LightCurves/extractLC.groovy  

# @livy
rm -rf LightCurves;hadoop fs -get /tmp/LightCurves

# @home
rm -rf ../data/LightCurves/src; scp -r julius.hrivnac@vm-75222.lal.in2p3.fr:/home/julius.hrivnac/Lomikel/ant/LightCurves ../data/LightCurves/src
rm -f ../data/LighCurves/all.csv; awk '(NR == 1) || (FNR > 1)' ../data/LightCurves/src/*csv > ../data/LightCurves/all.csv
lomikel_janus -s ../src/work/LightCurves/prepareLC.groovy -b

# visualisation ################################################################

# @home
python ../src/work/LightCurves/plotLC.py

for X in `ls ../run/LightCurves/*_2.idx`;do
  echo $X | sed 's\../run/LightCurves/\\g' | sed 's/_2.idx//g' | read Y 
  python ../src/work/LightCurves/plotLC.py $Y
  done

# classification ###############################################################

# @home
lomikel_dl4j -s ../src/work/LightCurves/trainLC.groovy -b

--------------------------------------------------------------------------------
'C*'  'QSO'

 0  1
-------
 25  4 | 0 = 0
  0 21 | 1 = 1


Accuracy:        0.9000
 Precision:       0.8077
 Recall:          1.0000
 F1 Score:        0.8936

Let’s break down the numbers you’ve provided from your deeplearning4j LSTM network’s curve recognition results for two classes. These metrics—accuracy, precision, recall, and F1 score—are standard ways to evaluate a classification model. Since you have two classes (let’s call them Class 0 and Class 1), and the output specifies that precision, recall, and F1 are reported for the "positive class" (Class 1), I’ll explain what each number means in that context.
# of classes: 2
This simply confirms your model is classifying data into two categories, like "yes/no" or "curve present/absent." No surprises here.
Accuracy: 0.9000 (90%)
Accuracy is the fraction of predictions your model got right overall. It’s calculated as (correct predictions) / (total predictions). A 90% accuracy means that out of all the samples (both Class 0 and Class 1), the model correctly classified 90% of them. This sounds good, but it doesn’t tell the whole story, especially if your classes are imbalanced (e.g., way more Class 0 than Class 1). That’s where the other metrics come in.
Precision: 0.8077 (80.77%)
Precision measures how trustworthy the model is when it predicts Class 1 (the positive class). It’s the ratio of true positives (correct Class 1 predictions) to all predicted positives (true positives + false positives). A precision of 80.77% means that when the model says "this is Class 1," it’s correct about 80.77% of the time. The other 19.23% are cases where it wrongly labeled Class 0 as Class 1. High precision is great if you want to avoid false alarms.
Recall: 1.0000 (100%)
Recall (also called sensitivity) measures how well the model catches all the actual Class 1 instances. It’s the ratio of true positives to all actual positives (true positives + false negatives). A recall of 100% means your model found every single instance of Class 1 in the dataset—it didn’t miss any. This is perfect recall, suggesting the model is extremely sensitive to Class 1, but it might come at the cost of over-predicting Class 1 (which ties into precision being lower).
F1 Score: 0.8936 (89.36%)
The F1 score is the harmonic mean of precision and recall, balancing the two. It’s useful when you care about both avoiding false positives (precision) and catching all positives (recall). An F1 of 89.36% is pretty strong—it’s closer to 1 than to 0—indicating a good trade-off between precision and recall. Since your recall is 100% but precision is 80.77%, the F1 score is pulled slightly down by the lower precision.
What’s Happening Here?
Your model is excellent at identifying all instances of Class 1 (100% recall), but it’s a bit overzealous—about 19% of the time it predicts Class 1, it’s actually wrong (precision of 80.77%). The overall accuracy of 90% suggests it’s doing well across both classes, and the F1 score of 89.36% shows a solid balance for Class 1. Since these metrics are only for Class 1, we don’t know how it performs on Class 0 without more data. If your dataset is balanced (roughly equal Class 0 and Class 1 samples), these numbers are very promising. If it’s imbalanced, you’d need to dig deeper into Class 0’s performance.
Does this align with what you expected from your LSTM? If you’ve got the confusion matrix or more details (like Class 0 metrics), I can refine the explanation further!
￼
￼
￼

