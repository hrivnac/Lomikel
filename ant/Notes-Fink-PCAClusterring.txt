# data preparation
# @janusgraph
work/preparePCA.gremlin
# @home
scp centos@134.158.74.85:'/tmp/*.csv' ./
scp *csv julius.hrivnac@vm-75222.lal.in2p3.fr:'/tmp'

# at Livy
git pull;hadoop fs -put -f ../src/work/PCA.py;hadoop fs -put -f PCA-sample.csv /tmp

# at JanusGraph
lomikel_hbase -s PCA.groovy  

# @home
scp julius.hrivnac@vm-75222.lal.in2p3.fr:/tmp/plot.png ./

--------------------------------------------------------------------------------
   
        {d:DR3Name=string, d:Plx=float, d:cdsxmatch=string, d:e_Plx=float, d:gcvs=string, d:mulens=double, d:nalerthist=integer, d:rf_kn_vs_nonkn=double, d:rf_snia_vs_nonia=double, d:roid=integer, d:snn_sn_vs_all=double, d:snn_snia_vs_nonia=double, d:tracklet=string, d:vsx=string, i:aimage=float, i:aimagerat=float, i:bimage=float, i:bimagerat=float, i:candid=long, i:chinr=float, i:chipsf=float, i:classtar=float, i:clrcoeff=float, i:clrcounc=float, i:clrmed=float, i:clrrms=float, i:dec=double, i:decnr=double, i:diffmaglim=float, i:distnr=float, i:distpsnr1=float, i:distpsnr2=float, i:distpsnr3=float, i:drb=float, i:drbversion=string, i:dsdiff=float, i:dsnrms=float, i:elong=float, i:exptime=float, i:fid=integer, i:field=integer, i:fink_broker_version=string, i:fink_science_version=string, i:fwhm=float, i:isdiffpos=string, i:jd=double, i:jdendhist=double, i:jdendref=double, i:jdstarthist=double, i:jdstartref=double, i:magap=float, i:magapbig=float, i:magdiff=float, i:magfromlim=float, i:maggaia=float, i:maggaiabright=float, i:magnr=float, i:magpsf=float, i:magzpsci=float, i:magzpscirms=float, i:magzpsciunc=float, i:mindtoedge=float, i:nbad=integer, i:ncovhist=integer, i:ndethist=integer, i:neargaia=float, i:neargaiabright=float, i:nframesref=integer, i:nid=integer, i:nmatches=integer, i:nmtchps=integer, i:nneg=integer, i:objectId=string, i:objectidps1=long, i:objectidps2=long, i:objectidps3=long, i:pdiffimfilename=string, i:pid=long, i:programid=integer, i:programpi=string, i:publisher=string, i:ra=double, i:ranr=double, i:rb=float, i:rbversion=string, i:rcid=integer, i:rfid=long, i:schemavsn=string, i:scorr=double, i:seeratio=float, i:sgmag1=float, i:sgmag2=float, i:sgmag3=float, i:sgscore1=float, i:sgscore2=float, i:sgscore3=float, i:sharpnr=float, i:sigmagap=float, i:sigmagapbig=float, i:sigmagnr=float, i:sigmapsf=float, i:simag1=float, i:simag2=float, i:simag3=float, i:sky=float, i:srmag1=float, i:srmag2=float, i:srmag3=float, i:ssdistnr=float, i:ssmagnr=float, i:ssnamenr=string, i:ssnrms=float, i:sumrat=float, i:szmag1=float, i:szmag2=float, i:szmag3=float, i:tblid=long, i:tooflag=integer, i:xpos=float, i:ypos=float, i:zpclrcov=float, i:zpmed=float}   nullnull
        
        
        
        url() + "/sessions/" + idSession + "/statements", "{\"code\":\"" + code + "\"}"


curl --header "Content-Type: application/json" \
  --request POST \
  --data '{"file":"/user/julius.hrivnac/test.py", "conf":{"spark.mesos.secret":"secret"}}' \
  http://134.158.75.222:21111/batches
  
curl --header "Content-Type: application/json" --request DELETE http://134.158.75.222:21111/batches/1739

  
  
FINK_PACKAGES=\
org.apache.hbase:hbase-shaded-mapreduce:2.2.7,\
org.slf4j:slf4j-log4j12:1.7.36,\
org.slf4j:slf4j-simple:1.7.36

# Other dependencies that I compiled for our versions:
FINK_JARS=\
${FINK_HOME}/libs/hbase-spark-hbase2.3.0_spark3.4.1_scala2.12.0_hadoop3.3.6.jar,\
${FINK_HOME}/libs/hbase-spark-protocol-shaded-hbase2.3.0_spark3.4.1_scala2.12.0_hadoop3.3.6. jar

curl --header "Content-Type: application/json" \
  --request POST \
  --data '{"file":"/user/julius.hrivnac/PCA.py", "pyFiles":["/home/janusgraph/Lomikel/ant/PCA.py"], "conf":{"spark.mesos.principal":          "lsst","spark.mesos.secret":            "secret","spark.mesos.role":              "lsst","spark.cores.max":               "8","spark.executor.cores":          "2","name":                          "livy-batch","spark.jars":                    "hdfs://134.158.75.222:8020/user/julius.hrivnac/hbase-spark-hbase2.3.0_spark3.4.1_scala2.12.0_hadoop3.3.6.jar,hdfs://134.158.75.222:8020/user/julius.hrivnac/hbase-spark-protocol-shaded-hbase2.3.0_spark3.4.1_scala2.12.0_hadoop3.3.6.jar","spark.jars.packages":           "org.apache.spark:spark-streaming-kafka-0-10-assembly_2.12:3.4.1,org.apache.spark:spark-sql-kafka-0-10_2.12:3.4.1,org.apache.spark:spark-avro_2.12:3.4.1,org.apache.hbase:hbase-shaded-mapreduce:2.2.7,org.slf4j:slf4j-log4j12:1.7.36,org.slf4j:slf4j-simple:1.7.36","spark.executor.memory":         "4g"}, "proxyUser":"livy"}' \
  http://134.158.75.222:21111/batches
  
   
---

3. Clustering (KMeans) Issues
a. Silhouette Score Calculation in a Loop
You perform multiple KMeans fits inside a loop (for i in range(5, 25)) to find the best number of clusters.
Problem: Each iteration re-trains KMeans, which is computationally expensive.
Optimization: Use Bisecting KMeans, which is more efficient and works better with large datasets:
python
￼Copy
￼Edit
from pyspark.ml.clustering import BisectingKMeans
bkm = BisectingKMeans().setK(n_clusters).setFeaturesCol(cluster_features)
model = bkm.fit(df_pca)